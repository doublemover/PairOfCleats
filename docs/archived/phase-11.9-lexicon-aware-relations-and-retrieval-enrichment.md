> **DEPRECATED**
> Replacement: `LEXI.md`
> Reason: Absorbed into LEXI roadmap.
> Date: 2026-02-03
> Commit: 75d70209
# Phase 11.9 -- Lexicon-Aware Relations and Retrieval Enrichment  > **Intent:** Add standardized per-language lexicons (wordlists) and integrate them into: > - **Build-time relation cleanup** (calls + usages) to reduce noise and improve `--calls/--uses` filtering. > - **Optional retrieval-time ranking boosts** (relation-aware, lexicon-aware) that are **boost-only** (no recall loss). > - **Optional chargram enrichment** (more signal fields) + **ANN candidate safety** (cap/fallback) so enrichment cannot reduce recall.  This phase is written to match the structure and operational detail level of `GIGAROADMAP.md`.  ---  ## Objective  - Introduce a **standardized, versioned lexicon wordlist per language** that can answer:   - “Is token `t` a **keyword** / **literal** / **builtin** / **primitive type** / **stdlib module** for language `L`?” - Use the lexicon to **clean up `rawRelations`** (calls/usages/callDetails) before building:   - `file_relations.json`   - `codeRelations.calls` (per chunk)   - `call_sites`-related artifacts (downstream) - Add **optional**, **boost-only** retrieval signals:   - Boost hits whose `codeRelations.calls` / `file_relations.usages` align with query tokens **excluding lexicon stopwords**. - Add **optional** chargram enrichment (index-time) while guaranteeing:   - No recall regressions (candidate set becomes “hint”, not “hard filter”).   - Candidate sets are **bounded** (cap/disable/fallback rules) and observable.  ---  ## Exit Criteria  - [ ] A lexicon exists for each language id in the active language registry (at least `keywords` + `literals`), and loads deterministically. - [ ] Build-time relation filtering:   - [ ] Removes only **lexicon-validated noise** by default (`keywords`, `literals`).   - [ ] Never breaks indexing and is fully **fail-open** (missing lexicon => no filtering).   - [ ] Has unit tests + fixture-based tests proving `--calls` and `--uses` filter correctness improvements. - [ ] Retrieval-time lexicon-aware boosts:   - [ ] Are **disabled by default** (or gated behind `quality=max`) and are **boost-only**.   - [ ] Have deterministic explain output and unit tests. - [ ] Chargram enrichment:   - [ ] Is **opt-in** (or gated behind `quality=max`).   - [ ] Includes ANN candidate cap/fallback behavior with tests proving it cannot reduce recall. - [ ] All new artifacts/specs have explicit contracts and validation tests.  ---  ## Specs Delivered With This Phase  Read these before coding; they remove ambiguity and define the contracts:  - `spec-language-lexicon-wordlists.md` - `language-lexicon-wordlist.schema.json` - `spec-lexicon-relations-filtering.md` - `spec-lexicon-retrieval-boosts.md` - `spec-chargram-enrichment-and-ann-fallback.md`  ---  ## Read-First File Map  These are the *actual* touchpoints in the current architecture; read in this order:  1. **Build-time relations + call index**    - `src/index/build/file-processor/cpu.js`      - Where `rawRelations` is produced and immediately converted into `fileRelations` and `callIndex`.    - `src/index/build/file-processor/relations.js`      - `buildFileRelations(rawRelations, relKey)`      - `buildCallIndex(rawRelations)`    - `src/index/build/file-processor/process-chunks.js`      - Creates per-chunk `codeRelations` from `callIndex` and writes call details per chunk index.  2. **Retrieval filters that consume relations**    - `src/retrieval/output/filters.js`      - `--calls` reads `chunk.codeRelations.calls`      - `--uses` reads `chunk.codeRelations.usages` (or file-level `file_relations.usages`)  3. **Retrieval pipeline scoring (ranking hooks)**    - `src/retrieval/pipeline.js`      - Where `symbolBoost` / `phraseBoost` exist today; this phase adds an **optional** `relationBoost`.  4. **Chargrams index-time + candidate set query-time**    - `src/index/build/state.js`      - How chargrams are emitted today (`name` + `doc` only, when `chargramSource=fields`)    - `src/shared/postings-config.js`      - Where to add `chargramFields` and `chargramStopwords` (index-time config)    - `src/retrieval/pipeline/candidates.js`      - Candidate set union logic (currently unbounded by `annCandidateCap`; this phase wires in caps/fallback rules).  5. **Build runtime wiring**    - `src/index/build/runtime/runtime.js`      - Where to plumb `languageOptions.lexicon` and any `postingsConfig` defaults for opt-in enrichment.  ---  ## Phase 11.9.1 -- Language Lexicon Assets and Loader  ### Objective  Provide a single, standardized lexicon surface that both build and retrieval can consume: - `getLanguageLexicon(languageId)` -> normalized sets - Deterministic caching and fail-open behavior - Strong schema validation for wordlist files  ### Goals - A lexicon file exists for every language id the registry can emit. - The loader is fast, cached, and does not allocate large structures repeatedly. - Lexicons are versioned so we can safely evolve the format.  ### Non-Goals - Replacing every language module’s bespoke skip sets (`TS_USAGE_SKIP`, `SHELL_CALL_KEYWORDS`, etc.) in this phase. - Perfect completeness of builtins / stdlib lists (keywords + literals are the only hard requirement for v1).  ### Touchpoints  - New:   - `src/lang/lexicon/index.js`   - `src/lang/lexicon/load.js`   - `src/lang/lexicon/normalize.js`   - `src/lang/lexicon/wordlists/*.json` (one per language id)   - `src/lang/lexicon/wordlists/_generic.json` (fallback) - Tests:   - `tests/lexicon/lexicon-loads-all-languages.test.js`   - `tests/lexicon/lexicon-schema.test.js`  ### Tasks  - [ ] **Add lexicon module skeleton**   - [ ] Create `src/lang/lexicon/index.js` exporting:     - [ ] `getLanguageLexicon(languageId, options?)`     - [ ] `isLexiconStopword(languageId, token, domain)` where `domain ∈ {relations, ranking, chargrams}`     - [ ] `extractSymbolBaseName(name)` helper (used by relations filtering + ranking)   - [ ] Create `src/lang/lexicon/load.js`:     - [ ] Resolve wordlist directory via `import.meta.url` (do **not** assume CWD).     - [ ] Load JSON files with `fs.readFile` + `JSON.parse`.     - [ ] Cache loaded lexicons in a module-level `Map<languageId, LanguageLexicon>`.     - [ ] Fail-open: missing file -> load `_generic.json`.   - [ ] Create `src/lang/lexicon/normalize.js`:     - [ ] Normalize arrays into `Set`s and enforce:       - [ ] lowercase       - [ ] unique       - [ ] sorted (for disk form; in-memory can be Set)       - [ ] ASCII only (unless explicitly allowed; v1: ASCII only)  - [ ] **Create v1 lexicon wordlist files**   - [ ] Create `_generic.json` with:     - [ ] `keywords`: minimal common programming keywords (if/for/while/return/etc.)     - [ ] `literals`: `true/false/null/nil/none/undefined`     - [ ] keep `builtins/types/modules` empty for generic   - [ ] Create per-language files matching language registry ids (minimum: keywords + literals):     - [ ] `javascript.json`     - [ ] `typescript.json`     - [ ] `python.json`     - [ ] `clike.json`     - [ ] `go.json`     - [ ] `java.json`     - [ ] `csharp.json`     - [ ] `kotlin.json`     - [ ] `ruby.json`     - [ ] `php.json`     - [ ] `lua.json`     - [ ] `sql.json`     - [ ] `perl.json`     - [ ] `rust.json`     - [ ] `swift.json`     - [ ] `shell.json`     - [ ] (Optional but recommended for completeness): the “simple” languages:       - [ ] `dockerfile.json`, `makefile.json`, `cmake.json`, `starlark.json`, `nix.json`, `dart.json`, `scala.json`, `groovy.json`, `r.json`, `julia.json`, `handlebars.json`, `mustache.json`, `jinja.json`, `razor.json`, `protobuf.json`, `graphql.json`  - [ ] **Add schema validation**   - [ ] Add `language-lexicon-wordlist.schema.json` (provided by this phase).   - [ ] Add a small validator in `src/lang/lexicon/load.js`:     - [ ] Validate required fields and types.     - [ ] On schema failure: log once per language and fall back to `_generic.json` (fail-open).  ### Tests  - [ ] `tests/lexicon/lexicon-schema.test.js`   - [ ] Load every wordlist file and validate against `language-lexicon-wordlist.schema.json`.   - [ ] Assert required arrays exist and are arrays of strings.   - [ ] Assert all entries are lowercase and unique. - [ ] `tests/lexicon/lexicon-loads-all-languages.test.js`   - [ ] Enumerate language ids from `src/index/language-registry/registry-data.js` (`LANGUAGE_REGISTRY.map(l => l.id)`).   - [ ] For each, ensure `getLanguageLexicon(id)` returns an object with `keywords` and `literals` Sets.   - [ ] Ensure unknown language id returns `_generic` fallback.  ---  ## Phase 11.9.2 -- Build-Time Lexicon-Aware Relation Filtering  ### Objective  Clean `rawRelations` *after extraction* (per-language parsers stay untouched) and *before* building: - `file_relations.json` (imports/exports/usages/importLinks/metadata) - `callIndex` (per-caller call lists + details)  ### Goals - Reduce noise in `usages` and `calls` without damaging general search recall. - Improve accuracy of CLI filters:   - `search --uses X`   - `search --calls X` - Provide metrics (counts) to tune lists safely.  ### Non-Goals - Changing tokenization or sparse postings (BM25) semantics. - Changing language relation extraction behavior; we only post-process outputs.  ### Touchpoints  - Modify:   - `src/index/build/file-processor/cpu.js` - New:   - `src/index/build/file-processor/lexicon-relations-filter.js` - Optional (metrics):   - `src/index/build/metrics/collector.js` (if you want structured counts; otherwise log-only)  ### Tasks  - [ ] **Implement a relations filter module**   - [ ] Create `src/index/build/file-processor/lexicon-relations-filter.js` exporting:     - [ ] `filterRawRelationsWithLexicon(rawRelations, { languageId, lexicon, config, log })`   - [ ] Implement filtering rules (per `spec-lexicon-relations-filtering.md`):     - [ ] `usages`: drop tokens that are in `lexicon.stopwords.relations` (default: keywords + literals)     - [ ] `calls`: for each `[caller, callee]`:       - [ ] compute `base = extractSymbolBaseName(callee)`       - [ ] drop if base is stopword for relations calls (default: keywords + literals)     - [ ] `callDetails` + `callDetailsWithRange`:       - [ ] drop entries whose `callee` base is stopword       - [ ] keep stable ordering     - [ ] De-dupe outputs where appropriate (but do not reorder unless deduping requires; prefer stable-set dedupe)   - [ ] Fail-open behavior:     - [ ] If lexicon unavailable or disabled -> return `rawRelations` unchanged.  - [ ] **Wire filtering into CPU file processing**   - [ ] In `src/index/build/file-processor/cpu.js`, locate:     - `const fileRelations = relationsEnabled ? buildFileRelations(rawRelations, relKey) : null;`     - `const callIndex = relationsEnabled ? buildCallIndex(rawRelations) : null;`   - [ ] Insert filtering directly before those lines:     - [ ] Determine `languageId = lang?.id || null`     - [ ] Determine `lexiconConfig = languageOptions?.lexicon || null`     - [ ] `rawRelationsFiltered = filterRawRelationsWithLexicon(rawRelations, { languageId, lexicon: languageOptions?.lexiconProvider, config: lexiconConfig, log })`     - [ ] Build `fileRelations` and `callIndex` from `rawRelationsFiltered`   - [ ] Ensure `languageOptions` exposes a `lexiconProvider` (or directly export `getLanguageLexicon` and call it here).     - Preferred: `languageOptions.lexiconProvider = { get(languageId): LanguageLexicon }` created once in runtime.  - [ ] **Runtime wiring**   - [ ] In `src/index/build/runtime/runtime.js`, extend `languageOptions`:     - [ ] `lexicon: { enabled: true, relations: { enabled: true, drop: { keywords: true, literals: true, builtins: false, types: false } } }`     - [ ] `lexiconProvider: createLexiconProvider(...)`   - [ ] Default behavior: enabled + only drops keywords/literals (safe, conservative).  ### Tests  - [ ] **Unit tests for filtering function**   - [ ] Add `tests/file-processor/lexicon-relations-filter.test.js`     - [ ] Provide synthetic `rawRelations` with:       - calls to `if`, `for`, `return`, `null`, `true`       - usages including `if`, `return`, `true`       - callDetails with `callee: "obj.default"` (ensure base `"default"` is *not* dropped unless listed as keyword for that language)     - [ ] Assert the default config drops only the intended items.     - [ ] Assert stable ordering of calls/callDetails is preserved.  - [ ] **Fixture-based integration test**   - [ ] Add `tests/retrieval/uses-and-calls-filters-respect-lexicon.test.js`     - [ ] Use `ensureFixtureIndex({ fixtureName: 'languages' })` from `tests/helpers/fixture-index.js`.     - [ ] Load `file_relations` via `loadFixtureIndexMeta(...)`.     - [ ] Pick one language fixture file (e.g. `src/python_advanced.py`) that contains clear builtin calls/usages.     - [ ] Assert `--uses` and `--calls` filters behave predictably:       - [ ] `--uses if` returns **0** results (or a stable small count if lexicon doesn’t treat it as stopword for that language).       - [ ] `--calls return` returns **0** results.       - [ ] `--calls print` still returns results (since builtins are not dropped by default).     - Note: this test must be aligned with the lexicon contents committed in phase 11.9.1.  ---  ## Phase 11.9.3 -- Retrieval-Time Lexicon-Aware Relation Boosts  ### Objective  Improve ranking quality by boosting results whose relations match query intent, while ensuring: - **No filtering** (no recall loss) - Deterministic and explainable scoring - Opt-in gating (default off or `quality=max` only)  ### Goals - Boost chunks where:   - Query tokens match `codeRelations.calls` callee base names, or   - Query tokens match file-level `usages` - Ignore lexicon stopwords (keywords/literals/builtins/types) for boosting decisions.  ### Non-Goals - Replacing BM25/ANN ranking architecture. - Cross-file graph ranking (that belongs in Phase 11.4+; this is local lexicon-based heuristic only).  ### Touchpoints  - Modify:   - `src/retrieval/pipeline.js`   - `src/retrieval/cli/normalize-options.js` (only if you add a new CLI/config surface; otherwise avoid) - New:   - `src/retrieval/scoring/relation-boost.js`   - `src/retrieval/scoring/lexicon-provider.js` (optional convenience wrapper) - Tests:   - `tests/retrieval/relation-boost.test.js`   - `tests/retrieval/relation-boost-does-not-filter.test.js`  ### Tasks  - [ ] **Implement relation boost module**   - [ ] Create `src/retrieval/scoring/relation-boost.js` exporting:     - [ ] `computeRelationBoost({ chunk, fileRelations, queryTokens, lexicon, config }) -> { boost: number, explain: object|null }`   - [ ] Matching logic (per `spec-lexicon-retrieval-boosts.md`):     - [ ] Determine `langId = chunk.lang || 'unknown'`     - [ ] Derive `signalTokens = queryTokens.filter(t => !lexicon.isStopword(langId, t, 'ranking'))`     - [ ] Build `callBaseSet` from `chunk.codeRelations.calls`:       - [ ] For each `[caller, callee]`, add `extractSymbolBaseName(callee)`     - [ ] Build `usageSet` from `chunk.codeRelations.usages` or `fileRelations.usages`     - [ ] Count matches:       - [ ] `callMatches = count(signalTokens ∩ callBaseSet)`       - [ ] `usageMatches = count(signalTokens ∩ usageSet)`     - [ ] Compute bounded boost:       - [ ] `boost = min(maxBoost, callMatches * perCall + usageMatches * perUse)`     - [ ] Provide explain payload:       - [ ] counts       - [ ] a truncated list of matched tokens (max N=10) - [ ] **Wire into pipeline scoring**   - [ ] In `src/retrieval/pipeline.js`, locate the per-hit scoring area:     - where `hit.score` gets updated by `symbolBoost`, `phraseBoost`, etc.   - [ ] Add:     - [ ] A config knob `relationBoostEnabled` (default false).     - [ ] If enabled, call `computeRelationBoost(...)` and add `boost` to `hit.score`.     - [ ] Add `hit.explain.relationBoost = explain` when `argv.explain` is enabled.   - [ ] Ensure **no filtering**:     - [ ] The candidate hit list length must never shrink due to boosting logic.  - [ ] **Gating**   - [ ] Default: disabled unless `quality=max` OR an explicit config flag is set.   - [ ] Avoid adding CLI flags in v1 unless necessary; prefer quality gating to prevent schema drift.     - If you do add flags, update docs/config schema and CLI schema drift tests accordingly.  ### Tests  - [ ] `tests/retrieval/relation-boost.test.js`   - [ ] Provide a synthetic `chunk` with:     - `lang: 'typescript'`     - `codeRelations.calls: [['foo', 'bar.baz'], ['foo', 'console.log']]`     - `codeRelations.usages: ['baz', 'qux']`   - [ ] Provide `queryTokens = ['baz', 'console', 'if']`   - [ ] Provide lexicon where `console` is builtin and `if` is keyword.   - [ ] Assert:     - [ ] `baz` contributes to boost     - [ ] `console` does **not** contribute (builtin stopword for ranking)     - [ ] `if` does **not** contribute - [ ] `tests/retrieval/relation-boost-does-not-filter.test.js`   - [ ] Construct a list of hits and run pipeline scoring step only.   - [ ] Assert hit count is unchanged regardless of boost enabled/disabled.  ---  ## Phase 11.9.4 -- Chargram Enrichment and ANN Candidate Safety  ### Objective  Optionally enrich chargram postings to improve candidate coverage for semantic retrieval, while making sure ANN candidate sets: - Are capped / disabled when unhelpful - Fall back when too restrictive - Never reduce recall (candidate set is a hint, not a hard constraint)  ### Goals - Allow chargrams to be built from additional fields (signature/comment) **without exploding** postings. - Ensure ANN candidate set behavior respects:   - `annCandidateCap`   - `annCandidateMinDocCount`   - `annCandidateMaxDocCount` - Add observability so we can tune thresholds safely.  ### Non-Goals - Rewriting the chargram indexing model entirely. - Making chargrams a required feature for search correctness.  ### Touchpoints  - Modify:   - `src/shared/postings-config.js`   - `src/index/build/state.js`   - `src/retrieval/pipeline.js`   - `src/retrieval/pipeline/candidates.js` (only if needed) - New:   - `src/retrieval/scoring/ann-candidate-policy.js` - Tests:   - `tests/postings/chargram-fields.test.js`   - `tests/retrieval/ann-candidate-policy.test.js`  ### Tasks  - [ ] **Extend postingsConfig**   - [ ] In `src/shared/postings-config.js`, add:     - [ ] `chargramFields: string[]` default `['name', 'doc']`     - [ ] `chargramStopwords: boolean` default `false`   - [ ] Ensure normalization clamps to known fields:     - Allowed: `name`, `signature`, `doc`, `comment`, `body`  - [ ] **Implement chargram fields selection**   - [ ] In `src/index/build/state.js` inside `appendChunk(...)`:     - [ ] In `chargramSource === 'fields'` branch, instead of hardcoding `name` and `doc`, read `config.chargramFields`.     - [ ] Iterate those fields and call `addFromTokens(state.fieldTokens[chunkId]?.[field])`.   - [ ] Optional stopword filtering (requires lexicon provider plumbed to `appendChunk`):     - [ ] If `config.chargramStopwords === true`:       - [ ] Drop tokens that are stopwords for `domain='chargrams'` for the chunk language.       - [ ] Default chargram stopwords should include keywords + literals, and optionally builtins/types (see spec).  - [ ] **ANN candidate safety policy**   - [ ] Create `src/retrieval/scoring/ann-candidate-policy.js` exporting:     - [ ] `resolveAnnCandidateSet({ candidates, bmHits, allowedIdx, filtersActive, config })`   - [ ] Implement policy:     - [ ] If `candidates == null` -> return `null` (means full ANN search)     - [ ] If `candidates.size === 0` -> return `new Set()` (means “no candidates” => no ANN hits)     - [ ] If `candidates.size > annCandidateCap` OR `> annCandidateMaxDocCount`:       - [ ] Return `null` (disable candidate filtering; full ANN search)     - [ ] If `candidates.size < annCandidateMinDocCount` and NOT `filtersActive`:       - [ ] Return `null` (too restrictive; full ANN search)     - [ ] If `filtersActive` and `allowedIdx` exists:       - [ ] Return `allowedIdx` (preserve filter semantics)     - [ ] Otherwise return `candidates`   - [ ] Wire into `src/retrieval/pipeline.js`:     - Replace current `annCandidateBase = candidates || ...` logic with policy output.  ### Tests  - [ ] `tests/postings/chargram-fields.test.js`   - [ ] Build a tiny in-memory `state` and append a chunk with fieldTokens for all fields.   - [ ] With default config, assert chargrams derive from `name` + `doc` only.   - [ ] With `chargramFields=['name','signature']`, assert signature tokens contribute chargrams. - [ ] `tests/retrieval/ann-candidate-policy.test.js`   - [ ] Construct candidate sets of varying sizes and assert:     - [ ] size > cap => policy returns null     - [ ] size < min and no filters => null     - [ ] filtersActive + allowedIdx => allowedIdx     - [ ] normal size => candidates unchanged  ---  ## Phase 11.9.5 -- Observability, Tuning, and Rollout  ### Objective  Make it safe to ship: - Measure how much filtering occurs and in which languages - Make boosts explainable and tunable - Ensure opt-in/quality gating is consistent  ### Goals - Metrics for “relations filtered count by category” - Retrieval explain output includes relationBoost + candidatePolicy decisions - A simple tuning harness using existing `tests/fixtures/languages`  ### Non-Goals - Full-blown benchmark suite expansion (keep minimal; add follow-ups later)  ### Touchpoints  - Modify:   - `src/index/build/file-processor/cpu.js` (log counters)   - `src/retrieval/pipeline.js` (explain additions)   - `src/shared/auto-policy.js` (optional: enable these features only on `quality=max`) - Tests:   - `tests/retrieval/explain-includes-relation-boost.test.js`  ### Tasks  - [ ] **Build-time metrics**   - [ ] In `filterRawRelationsWithLexicon`, compute counts:     - droppedCallsKeywords     - droppedCallsLiterals     - droppedUsagesKeywords     - droppedUsagesLiterals   - [ ] Emit via `log(...)` in a structured, grep-able form:     - `lexicon.relations.filtered language=<id> file=<relKey> callsDropped=<n> usagesDropped=<n>`   - [ ] (Optional) add to `metricsCollector` if needed.  - [ ] **Explain output**   - [ ] When `--explain` enabled:     - [ ] Add `hit.explain.relationBoost`     - [ ] Add `hit.explain.annCandidatePolicy` describing:       - original size       - chosen policy (null / allowedIdx / candidates)       - reason (tooSmall/tooLarge/filtersActive/etc.)  - [ ] **Rollout gating**   - [ ] Default gating suggestion:     - [ ] Build-time relation filtering: **enabled** (safe, conservative)     - [ ] Retrieval relation boost + chargram enrichment: **quality=max only**   - [ ] Encode in `src/shared/auto-policy.js` (optional) or in runtime/retrieval defaults.  ### Tests  - [ ] `tests/retrieval/explain-includes-relation-boost.test.js`   - [ ] Run `runSearch` helper with `--explain` on a fixture query.   - [ ] Assert response JSON includes `explain.relationBoost` when enabled.  ---  ## Notes / Guardrails  - **Fail-open everywhere**: lexicon integration must never make indexing/search fail due to missing or malformed wordlists. - **Boost-only** ranking changes: do not prune candidates based on lexicon (except ANN candidate policy which must fall back to full). - **Minimize public surface**: prefer quality gating over adding new CLI flags in v1. - **Keep compatibility**: new optional configs must have defaults that preserve current behavior. 
