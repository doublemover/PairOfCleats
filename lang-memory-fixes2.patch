diff -ruN a/src/index/build/file-processor/assemble.js b/src/index/build/file-processor/assemble.js
--- a/src/index/build/file-processor/assemble.js	2026-01-16 11:07:00.000000000 +0000
+++ b/src/index/build/file-processor/assemble.js	2026-01-16 11:59:06.450050202 +0000
@@ -33,7 +33,10 @@
   const weight = getFieldWeight(chunk, rel);
   const docText = typeof docmeta.doc === 'string' ? docmeta.doc : '';
   const fieldedEnabled = postingsConfig?.fielded !== false;
-  const fieldTokens = fieldedEnabled ? {
+  const wantsFieldTokens = fieldedEnabled
+    || postingsConfig?.chargramSource === 'fields'
+    || postingsConfig?.phraseSource === 'fields';
+  const fieldTokens = wantsFieldTokens ? {
     name: chunk.name ? buildTokenSequence({
       text: chunk.name,
       mode: tokenMode,
@@ -60,7 +63,7 @@
       }).tokens
       : [],
     comment: commentFieldTokens,
-    body: tokens
+    body: fieldedEnabled ? tokens : []
   } : null;
   const headline = getHeadline(chunk, tokens);
   const externalDocs = relationsEnabled
diff -ruN a/src/index/build/state.js b/src/index/build/state.js
--- a/src/index/build/state.js	2026-01-16 11:07:00.000000000 +0000
+++ b/src/index/build/state.js	2026-01-16 11:57:30.377853869 +0000
@@ -1,4 +1,4 @@
-import { extractNgrams, tri } from '../../shared/tokenize.js';
+import { tri } from '../../shared/tokenize.js';
 import { normalizePostingsConfig } from '../../shared/postings-config.js';
 
 const DEFAULT_POSTINGS_CONFIG = normalizePostingsConfig();
@@ -35,6 +35,47 @@
   }
 }
 
+/**
+ * Appends phrase n-grams for a token sequence to a postings map without
+ * materializing the full n-gram array.
+ *
+ * This significantly reduces transient allocation pressure compared to
+ * `extractNgrams(...)`, especially for long token sequences.
+ */
+function appendPhraseNgramsToPostingsMap(map, tokens, docId, minN, maxN) {
+  if (!map) return;
+  if (!Array.isArray(tokens) || tokens.length === 0) return;
+  const min = Number.isFinite(minN) ? minN : 2;
+  const max = Number.isFinite(maxN) ? maxN : 4;
+  if (min < 1 || max < min) return;
+
+  const len = tokens.length;
+  // For very short token sequences, nothing to do.
+  if (len < min) return;
+
+  const sep = '\u0001';
+  const maxSpan = Math.min(max, len);
+
+  for (let i = 0; i < len; i += 1) {
+    // Build incrementally: token[i], token[i]âtoken[i+1], ...
+    let key = '';
+    for (let n = 1; n <= maxSpan; n += 1) {
+      const j = i + n - 1;
+      if (j >= len) break;
+      const tok = tokens[j];
+      if (tok == null || tok === '') {
+        // Reset on empty tokens so we don't emit malformed n-grams.
+        key = '';
+        continue;
+      }
+      key = key ? `${key}${sep}${tok}` : String(tok);
+      if (n >= min) {
+        appendDocIdToPostingsMap(map, key, docId);
+      }
+    }
+  }
+}
+
 function *iteratePostingDocIds(posting) {
   if (posting == null) return;
   if (typeof posting === 'number') {
@@ -131,6 +172,14 @@
   if (!seq.length) return;
 
   const phraseEnabled = postingsConfig?.enablePhraseNgrams !== false;
+  const phraseMinN = Number.isFinite(postingsConfig?.phraseMinN)
+    ? postingsConfig.phraseMinN
+    : 2;
+  const phraseMaxN = Number.isFinite(postingsConfig?.phraseMaxN)
+    ? postingsConfig.phraseMaxN
+    : 4;
+  const phraseSource = postingsConfig?.phraseSource === 'full' ? 'full' : 'fields';
+
   const chargramEnabled = postingsConfig?.enableChargrams !== false;
   const fieldedEnabled = postingsConfig?.fielded !== false;
   const chargramSource = postingsConfig?.chargramSource === 'full' ? 'full' : 'fields';
@@ -139,11 +188,6 @@
     : Math.max(2, Math.floor(Number(postingsConfig.chargramMaxTokenLength)));
 
   state.totalTokens += seq.length;
-  const ngrams = phraseEnabled
-    ? (Array.isArray(chunk.ngrams) && chunk.ngrams.length
-      ? chunk.ngrams
-      : extractNgrams(seq, postingsConfig.phraseMinN, postingsConfig.phraseMaxN))
-    : [];
 
   const charSet = new Set();
   if (chargramEnabled) {
@@ -170,8 +214,10 @@
         addFromTokens(fields.name);
         addFromTokens(fields.doc);
         if (!charSet.size) {
-          // Fallback when no field tokens exist.
-          addFromTokens(seq);
+          // Intentionally emit no chargrams when no field tokens exist.
+          // Falling back to the full token stream defeats the purpose of
+          // bounding the chargram vocabulary and can create extremely large
+          // postings maps in multi-language repos.
         }
       } else {
         addFromTokens(seq);
@@ -196,8 +242,19 @@
   }
 
   if (phraseEnabled) {
-    for (const ng of ngrams) {
-      appendDocIdToPostingsMap(state.phrasePost, ng, chunkId);
+    if (phraseSource === 'full') {
+      appendPhraseNgramsToPostingsMap(state.phrasePost, seq, chunkId, phraseMinN, phraseMaxN);
+    } else {
+      const fields = chunk.fieldTokens || {};
+      // IMPORTANT: Do not fall back to the full token stream here. The entire
+      // point of the field-based strategy is to keep phrase vocabulary bounded,
+      // especially across many languages.
+      const phraseFields = ['name', 'signature', 'doc', 'comment'];
+      for (const field of phraseFields) {
+        const fieldTokens = Array.isArray(fields[field]) ? fields[field] : null;
+        if (!fieldTokens || !fieldTokens.length) continue;
+        appendPhraseNgramsToPostingsMap(state.phrasePost, fieldTokens, chunkId, phraseMinN, phraseMaxN);
+      }
     }
   }
   if (chargramEnabled) {
diff -ruN a/src/shared/postings-config.js b/src/shared/postings-config.js
--- a/src/shared/postings-config.js	2026-01-16 11:07:00.000000000 +0000
+++ b/src/shared/postings-config.js	2026-01-16 11:50:38.361752235 +0000
@@ -10,6 +10,7 @@
  *   chargramMaxN:number,
  *   chargramMaxTokenLength:number|null,
  *   chargramSource:string,
+ *   phraseSource:string,
  *   fielded:boolean
  * }}
  */
@@ -18,6 +19,16 @@
   const enablePhraseNgrams = cfg.enablePhraseNgrams !== false;
   const enableChargrams = cfg.enableChargrams !== false;
   const fielded = cfg.fielded !== false;
+
+  // Phrase n-grams are very high-cardinality when derived from the full token
+  // stream of source code. Default to deriving them from low-cardinality fields
+  // (name/signature/doc/comment) unless explicitly requested.
+  const phraseSourceRaw = typeof cfg.phraseSource === 'string'
+    ? cfg.phraseSource.trim().toLowerCase()
+    : '';
+  const phraseSource = ['full', 'fields'].includes(phraseSourceRaw)
+    ? phraseSourceRaw
+    : 'fields';
   const chargramSourceRaw = typeof cfg.chargramSource === 'string'
     ? cfg.chargramSource.trim().toLowerCase()
     : '';
@@ -56,6 +67,7 @@
     enableChargrams,
     phraseMinN: phraseRange.min,
     phraseMaxN: phraseRange.max,
+    phraseSource,
     chargramMinN: chargramRange.min,
     chargramMaxN: chargramRange.max,
     chargramMaxTokenLength,
