diff --git a/src/index/build/file-processor/assemble.js b/src/index/build/file-processor/assemble.js
index a6a0777..e780106 100644
--- a/src/index/build/file-processor/assemble.js
+++ b/src/index/build/file-processor/assemble.js
@@ -10,6 +10,7 @@ export function buildChunkPayload({
   relKey,
   ext,
   languageId,
+  chargramTokens,
   tokens,
   seq,
   codeRelations,
@@ -79,6 +80,9 @@ export function buildChunkPayload({
     name: chunk.name,
     tokens,
     seq,
+    ...(Array.isArray(chargramTokens) && chargramTokens.length
+      ? { chargramTokens }
+      : {}),
     codeRelations,
     docmeta,
     stats,
diff --git a/src/index/build/file-processor.js b/src/index/build/file-processor.js
index 9e815fb..177331b 100644
--- a/src/index/build/file-processor.js
+++ b/src/index/build/file-processor.js
@@ -615,8 +615,13 @@ export function createFileProcessor(options) {
         }
 
         let fieldChargramTokens = null;
-        if (tokenContext.chargramSource === 'fields') {
-          const fieldText = [c.name, docmeta?.doc].filter(Boolean).join(' ');
+        // NOTE: chargramSource="fields" is intended to restrict chargram postings
+        // to a small, high-signal subset (e.g., name + doc) rather than the full
+        // token sequence for the chunk body.
+        if (tokenContext.chargramEnabled && tokenContext.chargramSource === 'fields') {
+          // Include signature alongside name + doc so "fields" chargrams still
+          // capture high-signal identifiers even when a chunk has no name.
+          const fieldText = [c.name, c.signature, docmeta?.doc].filter(Boolean).join(' ');
           if (fieldText) {
             const fieldSeq = buildTokenSequence({
               text: fieldText,
@@ -639,7 +644,6 @@ export function createFileProcessor(options) {
               ext,
               file: relKey,
               size: fileStat.size,
-              chargramTokens: fieldChargramTokens,
               ...(workerDictOverride ? { dictConfig: workerDictOverride } : {})
             });
             const tokenDurationMs = Date.now() - tokenStart;
@@ -687,7 +691,6 @@ export function createFileProcessor(options) {
             mode: tokenMode,
             ext,
             context: tokenContext,
-            chargramTokens: fieldChargramTokens,
             buffers: tokenBuffers
           });
           const tokenDurationMs = Date.now() - tokenStart;
@@ -778,6 +781,8 @@ export function createFileProcessor(options) {
           languageId: fileLanguageId || lang?.id || null,
           tokens,
           seq,
+          // Used by postings construction to honor postings.chargramSource="fields".
+          chargramTokens: fieldChargramTokens,
           codeRelations,
           docmeta,
           stats,
diff --git a/src/index/build/state.js b/src/index/build/state.js
index 63f58f9..1df4074 100644
--- a/src/index/build/state.js
+++ b/src/index/build/state.js
@@ -1,9 +1,26 @@
-import { extractNgrams, tri } from '../../shared/tokenize.js';
 import { normalizePostingsConfig } from '../../shared/postings-config.js';
 
 const DEFAULT_POSTINGS_CONFIG = normalizePostingsConfig();
 const TOKEN_RETENTION_MODES = new Set(['full', 'sample', 'none']);
 
+const TRI_START = '⟬';
+const TRI_END = '⟭';
+
+const normalizePositiveInt = (value) => {
+  const parsed = Number(value);
+  if (!Number.isFinite(parsed)) return null;
+  const n = Math.floor(parsed);
+  return n > 0 ? n : null;
+};
+
+const buildNgramKey = (tokens, i, n) => {
+  // Common cases are small n (2..4). Avoid slice() allocations for those.
+  if (n === 2) return `${tokens[i]}_${tokens[i + 1]}`;
+  if (n === 3) return `${tokens[i]}_${tokens[i + 1]}_${tokens[i + 2]}`;
+  if (n === 4) return `${tokens[i]}_${tokens[i + 1]}_${tokens[i + 2]}_${tokens[i + 3]}`;
+  return tokens.slice(i, i + n).join('_');
+};
+
 // Postings maps can be extremely large (especially phrase n-grams and chargrams).
 // Storing posting lists as `Set`s is extremely memory-expensive when the vast
 // majority of terms are singletons (df=1).
@@ -133,39 +150,35 @@ export function appendChunk(
   const phraseEnabled = postingsConfig?.enablePhraseNgrams !== false;
   const chargramEnabled = postingsConfig?.enableChargrams !== false;
   const fieldedEnabled = postingsConfig?.fielded !== false;
+  const chunkId = state.chunks.length;
+
+  const maxPhraseNgramsPerDoc = normalizePositiveInt(
+    postingsConfig?.maxPhraseNgramsPerDoc
+    ?? postingsConfig?.maxNgramsPerDoc
+    ?? postingsConfig?.maxNgrams
+    ?? null
+  );
+  const maxChargramsPerDoc = normalizePositiveInt(
+    postingsConfig?.maxChargramsPerDoc
+    ?? postingsConfig?.maxNgramsPerDoc
+    ?? postingsConfig?.maxChargrams
+    ?? null
+  );
+
+  const chargramSourceRaw = typeof postingsConfig?.chargramSource === 'string'
+    ? postingsConfig.chargramSource.trim().toLowerCase()
+    : 'fields';
+  const chargramSource = chargramSourceRaw === 'full' ? 'full' : 'fields';
   const chargramMaxTokenLength = postingsConfig?.chargramMaxTokenLength == null
     ? null
     : Math.max(2, Math.floor(Number(postingsConfig.chargramMaxTokenLength)));
 
   state.totalTokens += seq.length;
-  const ngrams = phraseEnabled
-    ? (Array.isArray(chunk.ngrams) && chunk.ngrams.length
-      ? chunk.ngrams
-      : extractNgrams(seq, postingsConfig.phraseMinN, postingsConfig.phraseMaxN))
-    : [];
-
-  const charSet = new Set();
-  if (chargramEnabled) {
-    const chargrams = Array.isArray(chunk.chargrams) && chunk.chargrams.length
-      ? chunk.chargrams
-      : null;
-    if (chargrams) {
-      chargrams.forEach((g) => charSet.add(g));
-    } else {
-      seq.forEach((w) => {
-        if (chargramMaxTokenLength && w.length > chargramMaxTokenLength) return;
-        for (let n = postingsConfig.chargramMinN; n <= postingsConfig.chargramMaxN; ++n) {
-          tri(w, n).forEach((g) => charSet.add(g));
-        }
-      });
-    }
-  }
 
   const freq = new Map();
   tokens.forEach((t) => {
     freq.set(t, (freq.get(t) || 0) + 1);
   });
-  const chunkId = state.chunks.length;
 
   state.docLengths[chunkId] = tokens.length;
   for (const [tok, count] of freq.entries()) {
@@ -178,13 +191,82 @@ export function appendChunk(
   }
 
   if (phraseEnabled) {
-    for (const ng of ngrams) {
-      appendDocIdToPostingsMap(state.phrasePost, ng, chunkId);
+    let emitted = 0;
+    const hardLimit = maxPhraseNgramsPerDoc;
+    const existing = Array.isArray(chunk.ngrams) && chunk.ngrams.length ? chunk.ngrams : null;
+    if (existing) {
+      for (let i = 0; i < existing.length; i += 1) {
+        appendDocIdToPostingsMap(state.phrasePost, existing[i], chunkId);
+        emitted += 1;
+        if (hardLimit && emitted >= hardLimit) break;
+      }
+    } else {
+      const minN = Math.max(2, Math.floor(Number(postingsConfig.phraseMinN)));
+      const maxN = Math.max(minN, Math.floor(Number(postingsConfig.phraseMaxN)));
+      outerPhrase: for (let n = minN; n <= maxN; n += 1) {
+        const maxStart = seq.length - n;
+        for (let i = 0; i <= maxStart; i += 1) {
+          appendDocIdToPostingsMap(state.phrasePost, buildNgramKey(seq, i, n), chunkId);
+          emitted += 1;
+          if (hardLimit && emitted >= hardLimit) break outerPhrase;
+        }
+      }
     }
   }
+
   if (chargramEnabled) {
-    for (const tg of charSet) {
-      appendDocIdToPostingsMap(state.triPost, tg, chunkId);
+    const existing = Array.isArray(chunk.chargrams) && chunk.chargrams.length ? chunk.chargrams : null;
+    if (existing) {
+      let emitted = 0;
+      for (let i = 0; i < existing.length; i += 1) {
+        appendDocIdToPostingsMap(state.triPost, existing[i], chunkId);
+        emitted += 1;
+        if (maxChargramsPerDoc && emitted >= maxChargramsPerDoc) break;
+      }
+    } else {
+      const charSet = new Set();
+      const minN = Math.max(2, Math.floor(Number(postingsConfig.chargramMinN)));
+      const maxN = Math.max(minN, Math.floor(Number(postingsConfig.chargramMaxN)));
+      const hardLimit = maxChargramsPerDoc;
+
+      const addFromTokens = (sourceTokens) => {
+        for (let ti = 0; ti < sourceTokens.length; ti += 1) {
+          const w = sourceTokens[ti];
+          if (chargramMaxTokenLength && w.length > chargramMaxTokenLength) continue;
+          const s = `${TRI_START}${w}${TRI_END}`;
+          for (let n = minN; n <= maxN; n += 1) {
+            const maxStart = s.length - n;
+            for (let i = 0; i <= maxStart; i += 1) {
+              charSet.add(s.slice(i, i + n));
+              if (hardLimit && charSet.size >= hardLimit) return true;
+            }
+          }
+        }
+        return false;
+      };
+
+      if (chargramSource === 'fields') {
+        if (Array.isArray(chunk.chargramTokens) && chunk.chargramTokens.length) {
+          addFromTokens(chunk.chargramTokens);
+        } else if (fieldedEnabled && chunk.fieldTokens && typeof chunk.fieldTokens === 'object') {
+          const fields = chunk.fieldTokens;
+          const fieldOrder = ['name', 'signature', 'doc'];
+          for (let fi = 0; fi < fieldOrder.length; fi += 1) {
+            const arr = Array.isArray(fields[fieldOrder[fi]]) ? fields[fieldOrder[fi]] : [];
+            if (!arr.length) continue;
+            if (addFromTokens(arr)) break;
+          }
+        } else {
+          // chargramSource="fields" means we do not fall back to the full token
+          // sequence when field-derived tokens are unavailable.
+        }
+      } else {
+        addFromTokens(seq);
+      }
+
+      for (const gram of charSet) {
+        appendDocIdToPostingsMap(state.triPost, gram, chunkId);
+      }
     }
   }
 
@@ -236,6 +318,7 @@ export function appendChunk(
   }
   applyTokenRetention(chunk, tokenRetention);
   if (chunk.seq) delete chunk.seq;
+  if (chunk.chargramTokens) delete chunk.chargramTokens;
   if (chunk.chargrams) delete chunk.chargrams;
   if (chunk.fieldTokens) delete chunk.fieldTokens;
   // Phrase postings are authoritative; do not retain per-chunk n-grams in meta.
diff --git a/src/shared/postings-config.js b/src/shared/postings-config.js
index 16ee2d6..c5443fc 100644
--- a/src/shared/postings-config.js
+++ b/src/shared/postings-config.js
@@ -6,8 +6,10 @@
  *   enableChargrams:boolean,
  *   phraseMinN:number,
  *   phraseMaxN:number,
+ *   maxPhraseNgramsPerDoc:number|null,
  *   chargramMinN:number,
  *   chargramMaxN:number,
+ *   maxChargramsPerDoc:number|null,
  *   chargramMaxTokenLength:number|null,
  *   chargramSource:string,
  *   fielded:boolean
@@ -30,6 +32,11 @@ export function normalizePostingsConfig(input = {}) {
     if (!Number.isFinite(num)) return null;
     return Math.floor(num);
   };
+
+  const normalizeCap = (value) => {
+    const n = toInt(value);
+    return Number.isFinite(n) && n > 0 ? n : null;
+  };
   const normalizeRange = (minRaw, maxRaw, defaults) => {
     let min = toInt(minRaw);
     let max = toInt(maxRaw);
@@ -39,8 +46,21 @@ export function normalizePostingsConfig(input = {}) {
     return { min, max };
   };
 
-  const phraseRange = normalizeRange(cfg.phraseMinN, cfg.phraseMaxN, { min: 2, max: 4 });
-  const chargramRange = normalizeRange(cfg.chargramMinN, cfg.chargramMaxN, { min: 3, max: 5 });
+  // Back-compat: support older config keys (ngramMin/ngramMax, chargramMin/chargramMax).
+  const phraseMinRaw = cfg.phraseMinN ?? cfg.ngramMinN ?? cfg.ngramMin;
+  const phraseMaxRaw = cfg.phraseMaxN ?? cfg.ngramMaxN ?? cfg.ngramMax;
+  const chargramMinRaw = cfg.chargramMinN ?? cfg.chargramMin;
+  const chargramMaxRaw = cfg.chargramMaxN ?? cfg.chargramMax;
+
+  const phraseRange = normalizeRange(phraseMinRaw, phraseMaxRaw, { min: 2, max: 4 });
+  const chargramRange = normalizeRange(chargramMinRaw, chargramMaxRaw, { min: 3, max: 5 });
+
+  // Optional per-doc caps (null/undefined/<=0 means unlimited)
+  // - maxNgramsPerDoc: generic cap (applied to both phrase n-grams and chargrams unless overridden)
+  // - maxPhraseNgramsPerDoc / maxChargramsPerDoc: specific caps
+  const maxNgramsPerDoc = normalizeCap(cfg.maxNgramsPerDoc ?? cfg.maxNgramsPerChunk ?? cfg.maxNgrams);
+  const maxPhraseNgramsPerDoc = normalizeCap(cfg.maxPhraseNgramsPerDoc ?? cfg.maxPhraseNgrams) ?? maxNgramsPerDoc;
+  const maxChargramsPerDoc = normalizeCap(cfg.maxChargramsPerDoc ?? cfg.maxChargrams) ?? maxNgramsPerDoc;
   let chargramMaxTokenLength = 48;
   if (cfg.chargramMaxTokenLength === 0 || cfg.chargramMaxTokenLength === false) {
     chargramMaxTokenLength = null;
@@ -56,8 +76,10 @@ export function normalizePostingsConfig(input = {}) {
     enableChargrams,
     phraseMinN: phraseRange.min,
     phraseMaxN: phraseRange.max,
+    maxPhraseNgramsPerDoc,
     chargramMinN: chargramRange.min,
     chargramMaxN: chargramRange.max,
+    maxChargramsPerDoc,
     chargramMaxTokenLength,
     chargramSource,
     fielded
